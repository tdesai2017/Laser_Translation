{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class PixelExtractor:\n",
    "    def __init__(self, image_path, color = 'g'):\n",
    "        \"\"\"Image represents file path to target file \"\"\"\n",
    "        self.image_path = image_path\n",
    "        self.color = color\n",
    "        \n",
    "        \n",
    "    def imageprepare(self, argv):\n",
    "        \"\"\"\n",
    "        This function returns the pixel values as one array with 784 pixel values normalized\n",
    "        so that 255 is 1 and 0 is 0.\n",
    "        \"\"\"\n",
    "         \n",
    "        im = Image.open(argv).convert('L')\n",
    "        img = im.resize((28, 28), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        tv = list(img.getdata()) \n",
    "      \n",
    "        # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "        tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    "        return tva\n",
    "\n",
    "    def reshape_pixel_array(self, pixel_arr):\n",
    "        \"\"\" Takes flat array of 784 values and turns it into a 2d array with 28 rows of size 28 \"\"\"\n",
    "        reshaped_pixel_arr = []\n",
    "        n = 28\n",
    "        while n <= len(pixel_arr):\n",
    "            reshaped_pixel_arr.append(pixel_arr[n-28:n])\n",
    "            n+=28\n",
    "\n",
    "        return reshaped_pixel_arr\n",
    "    \n",
    "    def extract_target_pixel_location(self):\n",
    "        \"\"\" Returns list of target pixel locations \"\"\"\n",
    "        #Respective Image location\n",
    "        pixel_array = self.imageprepare(self.image_path)\n",
    "\n",
    "        #Select less_than_target color point --> must be calibrated\n",
    "        #?? Should we use an abstract class here instead of an if statment ??\n",
    "        if self.color == \"g\":\n",
    "            less_than_target = .15\n",
    "        else:\n",
    "            raise ValueError(\"Unknown color value\")\n",
    "\n",
    "        #Chooses target pixels as well as it's location\n",
    "        target_pixels = []\n",
    "        for pixel in enumerate(pixel_array):\n",
    "            if pixel[1] < less_than_target:\n",
    "                target_pixels.append(pixel[0])\n",
    "\n",
    "        return target_pixels\n",
    "    \n",
    "    def draw_image(self):\n",
    "        \"\"\" Draws the image representation of the rgb pixel valued image \"\"\"\n",
    "      \n",
    "        pixel_array = self.imageprepare(self.image_path)\n",
    "        newArr = self.reshape_pixel_array(pixel_array)\n",
    "        plt.imshow(newArr, interpolation='nearest')\n",
    "        plt.savefig('MNIST_IMAGE.png')#save MNIST image\n",
    "        plt.show()#Show / plot that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class ImageStitcher:\n",
    "    def __init__(self, directory_path = 'data'):\n",
    "        \"\"\" Directory Path represents directory containing spliced images \"\"\"\n",
    "\n",
    "        self.directory_path = directory_path\n",
    "        \n",
    "    def create_composite_image_list (self):\n",
    "        \"\"\" \n",
    "        Takes a directory and overlays its content images together based on \n",
    "        PixelExtractor's conditions\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        directory = os.fsencode(self.directory_path)\n",
    "        #255 represents a black cell\n",
    "        #0 represents a white cell\n",
    "        composite_image_list = [0 for i in range(784)]\n",
    "\n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "\n",
    "            #creates pixel_extractor instance\n",
    "            pixel_extractor = PixelExtractor(self.directory_path + '/' + filename, 'g')\n",
    "            target_pixel_locations = pixel_extractor.extract_target_pixel_location()\n",
    "            for loc in target_pixel_locations:\n",
    "                composite_image_list[loc] = 255\n",
    "\n",
    "        return composite_image_list\n",
    "    \n",
    "    def draw_image(self):\n",
    "        \"\"\"Draws the image representation of the composite image\"\"\"\n",
    "\n",
    "        composite_image_list = self.create_composite_image_list()\n",
    "\n",
    "        reshaped_composite_image = self.reshape_pixel_array(composite_image_list)\n",
    "\n",
    "        plt.imshow(reshaped_composite_image, cmap='Greys',  interpolation='nearest')\n",
    "        # plt.savefig('MNIST_IMAGE.png')#save MNIST image\n",
    "        plt.show()#Show / plot that image\n",
    "        return composite_image_list\n",
    "    \n",
    "    def reshape_pixel_array(self, composite_image_list):\n",
    "        \"\"\" Takes flat array of 784 values and turns it into a 2d array with 28 rows of size 28 \"\"\"\n",
    "        reshaped_composite_image = []\n",
    "        n = 28\n",
    "        while n <= len(composite_image_list):\n",
    "            reshaped_composite_image.append(composite_image_list[n-28:n])\n",
    "            n+=28\n",
    "\n",
    "        return reshaped_composite_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "class VideoToImageConverter:\n",
    "    \"\"\"\n",
    "    This class is responsible for converting a video to frames and saving it\n",
    "    to a specified directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_path):\n",
    "        self.video_path = video_path\n",
    "\n",
    "    def splice_video(self, destination_path = 'data'):\n",
    "        \"\"\" \n",
    "        Splices given video to individual images and \n",
    "        writes them to a folder specified by destination_path\n",
    "\n",
    "        *Don't name your destination path a folder that is important,\n",
    "        since this folder will be deleted and populated with images\n",
    "        \"\"\"\n",
    "\n",
    "        # Playing video from file:\n",
    "        vidcap = cv2.VideoCapture(self.video_path)\n",
    "\n",
    "        #Saves to respective folder\n",
    "        try:\n",
    "            if os.path.exists(destination_path):\n",
    "                #Deletes any folder currently named ./data if it exists\n",
    "                shutil.rmtree(destination_path, ignore_errors=True)\n",
    "                print(\"Deleting current '\" + destination_path + \"' folder\" )\n",
    "\n",
    "\n",
    "            print(\"Creating new '\" + destination_path + \"' folder\" )\n",
    "            os.makedirs(destination_path)\n",
    "\n",
    "        except OSError:\n",
    "            print ('Error: Cannot create directory')\n",
    "\n",
    "        frame_count = 0\n",
    "\n",
    "        while(True):\n",
    "            # Capture frame-by-frame\n",
    "\n",
    "            hasFrames,image = vidcap.read()\n",
    "\n",
    "            if hasFrames:\n",
    "\n",
    "                # Saves image of the current frame in jpg file\n",
    "                name = './'+ destination_path +'/frame' + str(frame_count) + '.jpg'\n",
    "                print ('Spliced ' + name)\n",
    "                cv2.imwrite(name, image)\n",
    "                frame_count += 1\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # When everything done, release the capture\n",
    "        vidcap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import joblib\n",
    "\n",
    "\n",
    "#only run this if you want to generate the model\n",
    "\n",
    "data = pd.read_csv(\"/Users/zumaad/Laser_Translation/final/train.csv\")\n",
    "print(\"made data\")\n",
    "print(\"\")\n",
    "features  = data.drop(\"label\", axis = 1)\n",
    "target = data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=3000)\n",
    "print(\"made training and test data\")\n",
    "print(\"\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "\n",
    "k1 = KNeighborsClassifier(n_neighbors = 4)\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth = 10)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "print(\"made models\")\n",
    "print(\"\")\n",
    "\n",
    "knn.fit(X=X_train, y=y_train)\n",
    "filename = 'Downloads/knn_model.sav'\n",
    "joblib.dump(knn, filename)\n",
    "print(\"fit KNeighborsClassifier\")\n",
    "print(\"\")\n",
    "print(\"saved the trained knn model\")\n",
    "print(\"\")\n",
    "\n",
    "#k1.fit(X=X_train, y=y_train)\n",
    "\n",
    "model.fit(X=X_train, y=y_train)\n",
    "print(\"fit DecisionTreeClassifier\")\n",
    "print(\"\")\n",
    "\n",
    "gnb.fit(X=X_train, y=y_train)\n",
    "print(\"fit GaussianNB\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"calculating accuracies:\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with KNeighborsClassifier :\", format(knn.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with KNeighborsClassifier :\", format(knn.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with DecisionTreeClassifier :\", format(model.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with DecisionTreeClassifier :\", format(model.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with GaussianNB :\", format(gnb.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with GaussianNB :\", format(gnb.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "# print(\"Prediction accuracy on the training data with k1 :\", format(k1.score(X_train, y_train)*100, \".2f\"))\n",
    "# print(\"Prediction accuracy on the test data with k1 :\", format(k1.score(X_test, y_test)*100, \".2f\"))\n",
    "# print(\"\")\n",
    "\n",
    "print(\"testing the model with singular data\")\n",
    "print(\"\")\n",
    "loaded_model = joblib.load(\"/Users/zumaad/Laser_Translation/final/k1_model.sav\")\n",
    "indexes start from 14322 and has 10,500 numbers for test data. \n",
    "indexes for eg are: 14322, 22264, 32118 etc.\n",
    "use same number for index in x_test  and y_test: y_test  are the  answers  for x_test.\n",
    "\n",
    "\n",
    "d = X_test.loc[[21284]]\n",
    "\n",
    "print(y_test[21284])\n",
    "print(\"\")\n",
    "\n",
    "model expects a data frame, the syntax above (passing a list to loc), returns a DF instead of a series representing a row\n",
    "print(loaded_model.predict(X_test.loc[[21284]])[0])\n",
    "print(\"\")\n",
    "\n",
    "if(y_test[21284] == loaded_model.predict(X_test.loc[[21284]])[0]):\n",
    "    print(\"test passed\")\n",
    "else:\n",
    "    print(\"test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model from the file with all the model properties\n",
    "loaded_model = joblib.load(\"/Users/zumaad/Laser_Translation/final/k1_model.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current 'data' folder\n",
      "Creating new 'data' folder\n",
      "Spliced ./data/frame0.jpg\n",
      "Spliced ./data/frame1.jpg\n",
      "Spliced ./data/frame2.jpg\n",
      "Spliced ./data/frame3.jpg\n",
      "Spliced ./data/frame4.jpg\n",
      "Spliced ./data/frame5.jpg\n",
      "Spliced ./data/frame6.jpg\n",
      "Spliced ./data/frame7.jpg\n",
      "Spliced ./data/frame8.jpg\n",
      "Spliced ./data/frame9.jpg\n",
      "Spliced ./data/frame10.jpg\n",
      "Spliced ./data/frame11.jpg\n",
      "Spliced ./data/frame12.jpg\n",
      "Spliced ./data/frame13.jpg\n",
      "Spliced ./data/frame14.jpg\n",
      "Spliced ./data/frame15.jpg\n",
      "Spliced ./data/frame16.jpg\n",
      "Spliced ./data/frame17.jpg\n",
      "Spliced ./data/frame18.jpg\n",
      "Spliced ./data/frame19.jpg\n",
      "Spliced ./data/frame20.jpg\n",
      "Spliced ./data/frame21.jpg\n",
      "Spliced ./data/frame22.jpg\n",
      "Spliced ./data/frame23.jpg\n",
      "Spliced ./data/frame24.jpg\n",
      "Spliced ./data/frame25.jpg\n",
      "Spliced ./data/frame26.jpg\n",
      "Spliced ./data/frame27.jpg\n",
      "Spliced ./data/frame28.jpg\n",
      "Spliced ./data/frame29.jpg\n",
      "Spliced ./data/frame30.jpg\n",
      "Spliced ./data/frame31.jpg\n",
      "Spliced ./data/frame32.jpg\n",
      "Spliced ./data/frame33.jpg\n",
      "Spliced ./data/frame34.jpg\n",
      "Spliced ./data/frame35.jpg\n",
      "Spliced ./data/frame36.jpg\n",
      "Spliced ./data/frame37.jpg\n",
      "Spliced ./data/frame38.jpg\n",
      "Spliced ./data/frame39.jpg\n",
      "Spliced ./data/frame40.jpg\n",
      "Spliced ./data/frame41.jpg\n",
      "Spliced ./data/frame42.jpg\n",
      "Spliced ./data/frame43.jpg\n",
      "Spliced ./data/frame44.jpg\n",
      "Spliced ./data/frame45.jpg\n",
      "Spliced ./data/frame46.jpg\n",
      "Spliced ./data/frame47.jpg\n",
      "Spliced ./data/frame48.jpg\n",
      "Spliced ./data/frame49.jpg\n",
      "Spliced ./data/frame50.jpg\n",
      "Spliced ./data/frame51.jpg\n",
      "Spliced ./data/frame52.jpg\n",
      "Spliced ./data/frame53.jpg\n",
      "Spliced ./data/frame54.jpg\n",
      "Spliced ./data/frame55.jpg\n",
      "Spliced ./data/frame56.jpg\n",
      "Spliced ./data/frame57.jpg\n",
      "Spliced ./data/frame58.jpg\n",
      "Spliced ./data/frame59.jpg\n",
      "Spliced ./data/frame60.jpg\n",
      "Spliced ./data/frame61.jpg\n",
      "Spliced ./data/frame62.jpg\n",
      "Spliced ./data/frame63.jpg\n",
      "Spliced ./data/frame64.jpg\n",
      "Spliced ./data/frame65.jpg\n",
      "Spliced ./data/frame66.jpg\n",
      "Spliced ./data/frame67.jpg\n",
      "Spliced ./data/frame68.jpg\n",
      "Spliced ./data/frame69.jpg\n",
      "Spliced ./data/frame70.jpg\n",
      "Spliced ./data/frame71.jpg\n",
      "Spliced ./data/frame72.jpg\n",
      "Spliced ./data/frame73.jpg\n",
      "Spliced ./data/frame74.jpg\n",
      "Spliced ./data/frame75.jpg\n",
      "Spliced ./data/frame76.jpg\n",
      "Spliced ./data/frame77.jpg\n",
      "Spliced ./data/frame78.jpg\n",
      "Spliced ./data/frame79.jpg\n",
      "Spliced ./data/frame80.jpg\n",
      "Spliced ./data/frame81.jpg\n",
      "Spliced ./data/frame82.jpg\n",
      "Spliced ./data/frame83.jpg\n",
      "Spliced ./data/frame84.jpg\n",
      "Spliced ./data/frame85.jpg\n",
      "Spliced ./data/frame86.jpg\n",
      "Spliced ./data/frame87.jpg\n",
      "Spliced ./data/frame88.jpg\n",
      "Spliced ./data/frame89.jpg\n",
      "Spliced ./data/frame90.jpg\n",
      "Spliced ./data/frame91.jpg\n",
      "Spliced ./data/frame92.jpg\n",
      "Spliced ./data/frame93.jpg\n",
      "Spliced ./data/frame94.jpg\n",
      "Spliced ./data/frame95.jpg\n",
      "Spliced ./data/frame96.jpg\n",
      "Spliced ./data/frame97.jpg\n",
      "Spliced ./data/frame98.jpg\n",
      "Spliced ./data/frame99.jpg\n",
      "Spliced ./data/frame100.jpg\n",
      "Spliced ./data/frame101.jpg\n",
      "Spliced ./data/frame102.jpg\n",
      "Spliced ./data/frame103.jpg\n",
      "Spliced ./data/frame104.jpg\n",
      "Spliced ./data/frame105.jpg\n",
      "Spliced ./data/frame106.jpg\n",
      "Spliced ./data/frame107.jpg\n",
      "Spliced ./data/frame108.jpg\n",
      "Spliced ./data/frame109.jpg\n",
      "Spliced ./data/frame110.jpg\n",
      "Spliced ./data/frame111.jpg\n",
      "Spliced ./data/frame112.jpg\n",
      "Spliced ./data/frame113.jpg\n",
      "Spliced ./data/frame114.jpg\n",
      "Spliced ./data/frame115.jpg\n",
      "Spliced ./data/frame116.jpg\n",
      "Spliced ./data/frame117.jpg\n",
      "Spliced ./data/frame118.jpg\n",
      "Spliced ./data/frame119.jpg\n",
      "Spliced ./data/frame120.jpg\n",
      "Spliced ./data/frame121.jpg\n",
      "Spliced ./data/frame122.jpg\n",
      "Spliced ./data/frame123.jpg\n",
      "Spliced ./data/frame124.jpg\n",
      "Spliced ./data/frame125.jpg\n",
      "Spliced ./data/frame126.jpg\n",
      "Spliced ./data/frame127.jpg\n",
      "Spliced ./data/frame128.jpg\n",
      "Spliced ./data/frame129.jpg\n",
      "Spliced ./data/frame130.jpg\n",
      "Spliced ./data/frame131.jpg\n",
      "Spliced ./data/frame132.jpg\n",
      "Spliced ./data/frame133.jpg\n",
      "Spliced ./data/frame134.jpg\n",
      "Spliced ./data/frame135.jpg\n",
      "Spliced ./data/frame136.jpg\n",
      "Spliced ./data/frame137.jpg\n",
      "Spliced ./data/frame138.jpg\n",
      "Spliced ./data/frame139.jpg\n",
      "Spliced ./data/frame140.jpg\n",
      "Spliced ./data/frame141.jpg\n",
      "Spliced ./data/frame142.jpg\n",
      "Spliced ./data/frame143.jpg\n",
      "Spliced ./data/frame144.jpg\n",
      "Spliced ./data/frame145.jpg\n",
      "Spliced ./data/frame146.jpg\n",
      "Spliced ./data/frame147.jpg\n",
      "Spliced ./data/frame148.jpg\n",
      "Spliced ./data/frame149.jpg\n",
      "Spliced ./data/frame150.jpg\n",
      "Spliced ./data/frame151.jpg\n",
      "Spliced ./data/frame152.jpg\n",
      "Spliced ./data/frame153.jpg\n",
      "Spliced ./data/frame154.jpg\n",
      "Spliced ./data/frame155.jpg\n",
      "Spliced ./data/frame156.jpg\n",
      "Spliced ./data/frame157.jpg\n",
      "Spliced ./data/frame158.jpg\n",
      "Spliced ./data/frame159.jpg\n",
      "Spliced ./data/frame160.jpg\n",
      "Spliced ./data/frame161.jpg\n",
      "Spliced ./data/frame162.jpg\n",
      "Spliced ./data/frame163.jpg\n",
      "Spliced ./data/frame164.jpg\n",
      "Spliced ./data/frame165.jpg\n",
      "Spliced ./data/frame166.jpg\n",
      "Spliced ./data/frame167.jpg\n",
      "Spliced ./data/frame168.jpg\n",
      "Spliced ./data/frame169.jpg\n"
     ]
    }
   ],
   "source": [
    "#convert video to frames to be analyzed to create the composite image\n",
    "vti_converter = VideoToImageConverter('5.mp4')\n",
    "vti_converter.splice_video('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object which is used to stitch individual frames into the composite image\n",
    "image_stitcher = ImageStitcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "pixel_array = image_stitcher.draw_image()\n",
    "pixel_dataframe = pd.DataFrame([pixel_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
