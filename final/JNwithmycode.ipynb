{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> DS 3000 - Spring 2019</h2> </center>\n",
    "<center> <h3> DS Report </h3> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h3> Lazer Translation</h3> </center>\n",
    "<center><h4>Srinath, Zumaad, Tushar, Eric</h4></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executive Summary:\n",
    "\n",
    "Our project was concerned with converting a video of someone drawing on a surface with a laser into a digit. To do this we had to convert a video into a representation of a digit that our model could understand. This involved splitting the video into frames/pixel arrays and creating an algorithm to create a composite pixel array which represented all the points the laser was at during the video.\n",
    "\n",
    "This composite pixel array was then passed to the model for prediction to get a digit. Our model was trained using KNN on a training set with around 30k images of digits and performed with 96.51% accuracy on the testing set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. <a href='#1'>INTRODUCTION</a>\n",
    "2. <a href='#2'>METHOD</a>\n",
    "3. <a href='#3'>RESULTS</a>\n",
    "4. <a href='#4'>DISCUSSION</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a proof of concept project that would allow us to convert laser drawn writing into digit representations. Through the completion of this endeavour we would have gained an in-depth exposure to python ski-kit lean’s utilities, have a greater understanding of using data sets to train models to recognize patterns such as handwriting, and achieve a comprehensive understanding of other python libraries such as image processing and pixel analysis. \n",
    "\n",
    "### Significance of the Problem\n",
    "Why is it important to tackle this problem in your project?\n",
    "In what ways could the insights from this project be useful?\n",
    "Has there been previous work on your topic? Do some research into your topic. Cite your sources appropriately. You can use the numbered reference format or APA (if you are more comfortable with it).\n",
    "This project serves as a proof of concept regarding the futuristic ability of medium translation from lasers to understandable text. Having a constant stream of data being read from a laser pointer and translated onto a screen not only greatly studies data science, machine learning, and image processing libraries, but will create an incredibly interesting project that can be constantly tweaked and improved. For example, one can extend the laser translation functionality to incorporate deciphering more than just digits - including symbols, shapes, and letters - and potentially bring light to a unique form of discrete communication. \n",
    "There has been no recorded research on our particular laser to text proposal. However, digit recognition is an incredibly widely researched subject, and one that many continue to discover new applications for [1]. We believe that our project signifies the creation of just one more of these useful applications.\n",
    "1. Team, D. F. (2020, January 7). Deep Learning Project - Handwritten Digit Recognition using\n",
    "Python. Retrieved from \n",
    "https://data-flair.training/blogs/python-deep-learning-project-handwritten-digit-recognition\n",
    "\n",
    "### Questions/Hypothesis\n",
    "\n",
    "\n",
    "Given the aforementioned problem and its importance, we set out to tackle the following questions:  \n",
    "\n",
    "Q1: Is the accuracy of our tests dependent on the robustness of our image stitching code or rather the model?  \n",
    "\n",
    "Q2: Will the speed of our laser drawn digit result in greater inaccuracy of digit prediction? (Requirement 2: variable question)  \n",
    "\n",
    "Q3: Due the incredible size of the data set, what is the optimal algorithm and what are the most effective parameters for this algorithm, e.g., choosing the number of iterations, error tolerance, etc.?  \n",
    "\n",
    "\n",
    "We believe that the accuracy will be dependent on the color of laser used and the color of the background wall as well - making these decisions integral in guaranteeing success. In order to prove this, we will run our program through different wall backgrounds and analyze the implications of each.  \n",
    "\n",
    "Our hypothesis is that the accuracy of our tests will be fully determined by how robust our image stitching (i.e. the process that takes the parsed video images and re-constitutes a composite image by analyzing the points in each image with the greatest color contrast) code proves to be - as opposed to it being an issue on the model’s side. Our workflow will be to capture a video of someone outlining a digit with a laser pointer, splitting this video into various images each with their own laser location, stitching all of these images back together to form a singular number made up of various laser point locations, feeding this data into our digit-recognizer model, and finally printing this number.  Thus, our hypothesis is that the accuracy of this methodology will ultimately define the accuracy of our model and code as a whole.    \n",
    "\n",
    "Furthermore, we believe that the speed of the drawing will be a determining factor regarding the ultimate prediction accuracy of the drawn digit. Since we will be parsing the video drawing by frames, it would make sense that writer slower equates more accurate results.  \n",
    "\n",
    "Since numbers have a high variance we have a very large dataset, we hypothesis that the KNN algorithm will provide the most optimal results. Furthermore, the large number of features usually bogs down many accuracy algorithms and and makes training time unfeasibly long. This is the case with LinearSVC and not the case with KNN, so we have further reason to believe that KNN will be an optimal algorithm choice.\n",
    "\n",
    "\n",
    "\n",
    "…..\n",
    "\n",
    "*Side note regarding question requirements - Our feature variables are 784 pixels, and in of themselves, they don’t have any particular meaning. For example, in a heart disease prediction machine project - one could draw hypotheses based on a single variable (e.g. body-fat percentage) and what they believe its impact will be on the result. For our case, variable-based questions will have to reference outside factors such as speed of drawing, color of background, etc, since a single pixel density does not provide insight in of itself.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Acquisition\n",
    "\n",
    "Training data: kaggle.com/c/digit-recognizer/download/G4erCQmLsLsmveFfJfNg%2Fversions%2FFy8gTgxUjCWjkk6UfEsa%2Ffiles%2Ftrain.csv  \n",
    "\n",
    "Testing data: kaggle.com/c/digit-recognizer/download/G4erCQmLsLsmveFfJfNg%2Fversions%2FFy8gTgxUjCWjkk6UfEsa%2Ffiles%2Ftest.csv  \n",
    "\n",
    "The dataset contains 28,000 digit image representations ranging from 1 through 9 - each represented through a collection of 784 pixel values labeled pixel0, pixel1..pixel783. Each pixel variable provides a grayscale value (ranging from 0 to 255) that makes up the image. By taking one of these 784 long pixel series, reshaping it into a 28x28 image, and plotting the data through pyplot, the image representation of the digit is created. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Variables\n",
    "* For your hypotheses, what are your IVs and DVs?\n",
    "* For your predictive models, what are your features and target variables?\n",
    "\n",
    "**Independant variables**: Speed of the drawing, color of the laser, and the color of the wall.  \n",
    "**Dependant variables**: Accuracy of the prediction.\n",
    "\n",
    "**Features**: 784 pixels which have values ranging from 0-255 inclusive, where 255 is black and 0 is white.  \n",
    "**Target variables**: The digit (1-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Data Analysis\n",
    "Our target variable is the digit that the pixel array represents and it's value is based on the 784 feature variables.\n",
    "\n",
    "This is a supervised learning problem as our model was trained on labeled data (digits each corresponding to a 784 pixel values). The learning task was classification as it is predicting between a set number of target classes and not an infinite number as is done during regression tasks.\n",
    "\n",
    "We are going to use KNN, as its a high variance classifer and works best with larger datasets andafter testing the accuracies KNN had the highest accuracy. Furthermore, since we have a lot of features, other classification algorithms like linear SVC take too long to train relative to KNN. \n",
    "\n",
    "KNN also demonstrated the optimal accuracy among all algorithms after tuning it with GridSearch (using parameters ranging from 1-10 and metrics ranging from Euclidean to Minowski) - in which demonstrated a 98% training accurace and a 96.5% testing accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Wrangling\n",
    "When training the model, we already had well formed data in a csv format that we could just read into a dataframe and train and test the model on. However, using the model with the rest of our workflow, we had to ultimately convert a video to a dataframe with 784 columns with each number in the column ranging from 0-255 where 255 is darkest and 0 is lightest, as that was the type of input the model needed. Doing this required extensive data wrangling.  \n",
    "\n",
    "To start off, we have a video of us drawing with a laser. We want to convert this to what represents an image of a digit. First, we have to split the video into frames and convert all of them into one image. Each frame shows the laser’s dot on the wall at a given time. So, to get the image of the digit we have to essentially overlay the laser point in each frame onto the final composite frame. To do this, we resize each frame so thats its 28 * 28 pixels (this is 784 pixels in totals like how the model expects). \n",
    "Then we get the pixel array of the image (a list of 784 items that range from 0-1, where 1 is darkest and 0 is lightest). To figure out the location of the laser point in that frame, we have a threshold: A green laser typically has a brightness less than .15, so we gather the locations of the pixels that are below that threshold and return them. These locations will be marked on the final composite pixel array as 255 (darkest) to mark where the laser’s point was.  \n",
    "\n",
    "After this process is done on every frame, the final composite pixel array has values 255 for every location the laser was, and 0 for all the other values. This array represents the digit. Finally, we convert it into a dataframe and pass it the model for prediction.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing of Video to Composite Pixel Image Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class PixelExtractor:\n",
    "    def __init__(self, image_path, color = 'g'):\n",
    "        \"\"\"Image represents file path to target file \"\"\"\n",
    "        self.image_path = image_path\n",
    "        self.color = color\n",
    "        \n",
    "        \n",
    "    def imageprepare(self, argv):\n",
    "        \"\"\"\n",
    "        This function returns the pixel values as one array with 784 pixel values normalized\n",
    "        so that 255 is 1 and 0 is 0.\n",
    "        \"\"\"\n",
    "         \n",
    "        im = Image.open(argv).convert('L')\n",
    "        img = im.resize((28, 28), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        tv = list(img.getdata()) \n",
    "      \n",
    "        # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "        tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    "        return tva\n",
    "\n",
    "    def reshape_pixel_array(self, pixel_arr):\n",
    "        \"\"\" Takes flat array of 784 values and turns it into a 2d array with 28 rows of size 28 \"\"\"\n",
    "        reshaped_pixel_arr = []\n",
    "        n = 28\n",
    "        while n <= len(pixel_arr):\n",
    "            reshaped_pixel_arr.append(pixel_arr[n-28:n])\n",
    "            n+=28\n",
    "\n",
    "        return reshaped_pixel_arr\n",
    "    \n",
    "    def extract_target_pixel_location(self):\n",
    "        \"\"\" Returns list of target pixel locations \"\"\"\n",
    "        #Respective Image location\n",
    "        pixel_array = self.imageprepare(self.image_path)\n",
    "\n",
    "        #Select less_than_target color point --> must be calibrated\n",
    "        #?? Should we use an abstract class here instead of an if statment ??\n",
    "        if self.color == \"g\":\n",
    "            less_than_target = .15\n",
    "        else:\n",
    "            raise ValueError(\"Unknown color value\")\n",
    "\n",
    "        #Chooses target pixels as well as it's location\n",
    "        target_pixels = []\n",
    "        for pixel in enumerate(pixel_array):\n",
    "            if pixel[1] < less_than_target:\n",
    "                target_pixels.append(pixel[0])\n",
    "\n",
    "        return target_pixels\n",
    "    \n",
    "    def draw_image(self):\n",
    "        \"\"\" Draws the image representation of the rgb pixel valued image \"\"\"\n",
    "      \n",
    "        pixel_array = self.imageprepare(self.image_path)\n",
    "        newArr = self.reshape_pixel_array(pixel_array)\n",
    "        plt.imshow(newArr, interpolation='nearest')\n",
    "        plt.savefig('MNIST_IMAGE.png')#save MNIST image\n",
    "        plt.show()#Show / plot that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class ImageStitcher:\n",
    "    def __init__(self, directory_path = 'data'):\n",
    "        \"\"\" Directory Path represents directory containing spliced images \"\"\"\n",
    "\n",
    "        self.directory_path = directory_path\n",
    "        \n",
    "    def create_composite_image_list (self):\n",
    "        \"\"\" \n",
    "        Takes a directory and overlays its content images together based on \n",
    "        PixelExtractor's conditions\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        directory = os.fsencode(self.directory_path)\n",
    "        #255 represents a black cell\n",
    "        #0 represents a white cell\n",
    "        composite_image_list = [0 for i in range(784)]\n",
    "\n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "\n",
    "            #creates pixel_extractor instance\n",
    "            pixel_extractor = PixelExtractor(self.directory_path + '/' + filename, 'g')\n",
    "            target_pixel_locations = pixel_extractor.extract_target_pixel_location()\n",
    "            for loc in target_pixel_locations:\n",
    "                composite_image_list[loc] = 255\n",
    "\n",
    "        return composite_image_list\n",
    "    \n",
    "    def draw_image(self):\n",
    "        \"\"\"Draws the image representation of the composite image\"\"\"\n",
    "\n",
    "        composite_image_list = self.create_composite_image_list()\n",
    "\n",
    "        reshaped_composite_image = self.reshape_pixel_array(composite_image_list)\n",
    "\n",
    "        plt.imshow(reshaped_composite_image, cmap='Greys',  interpolation='nearest')\n",
    "        # plt.savefig('MNIST_IMAGE.png')#save MNIST image\n",
    "        plt.show()#Show / plot that image\n",
    "        return composite_image_list\n",
    "    \n",
    "    def reshape_pixel_array(self, composite_image_list):\n",
    "        \"\"\" Takes flat array of 784 values and turns it into a 2d array with 28 rows of size 28 \"\"\"\n",
    "        reshaped_composite_image = []\n",
    "        n = 28\n",
    "        while n <= len(composite_image_list):\n",
    "            reshaped_composite_image.append(composite_image_list[n-28:n])\n",
    "            n+=28\n",
    "\n",
    "        return reshaped_composite_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "class VideoToImageConverter:\n",
    "    \"\"\"\n",
    "    This class is responsible for converting a video to frames and saving it\n",
    "    to a specified directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_path):\n",
    "        self.video_path = video_path\n",
    "\n",
    "    def splice_video(self, destination_path = 'data'):\n",
    "        \"\"\" \n",
    "        Splices given video to individual images and \n",
    "        writes them to a folder specified by destination_path\n",
    "\n",
    "        *Don't name your destination path a folder that is important,\n",
    "        since this folder will be deleted and populated with images\n",
    "        \"\"\"\n",
    "\n",
    "        # Playing video from file:\n",
    "        vidcap = cv2.VideoCapture(self.video_path)\n",
    "\n",
    "        #Saves to respective folder\n",
    "        try:\n",
    "            if os.path.exists(destination_path):\n",
    "                #Deletes any folder currently named ./data if it exists\n",
    "                shutil.rmtree(destination_path, ignore_errors=True)\n",
    "                print(\"Deleting current '\" + destination_path + \"' folder\" )\n",
    "\n",
    "\n",
    "            print(\"Creating new '\" + destination_path + \"' folder\" )\n",
    "            os.makedirs(destination_path)\n",
    "\n",
    "        except OSError:\n",
    "            print ('Error: Cannot create directory')\n",
    "\n",
    "        frame_count = 0\n",
    "\n",
    "        while(True):\n",
    "            # Capture frame-by-frame\n",
    "\n",
    "            hasFrames,image = vidcap.read()\n",
    "\n",
    "            if hasFrames:\n",
    "\n",
    "                # Saves image of the current frame in jpg file\n",
    "                name = './'+ destination_path +'/frame' + str(frame_count) + '.jpg'\n",
    "                print ('Spliced ' + name)\n",
    "                cv2.imwrite(name, image)\n",
    "                frame_count += 1\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # When everything done, release the capture\n",
    "        vidcap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data Exploration\n",
    "* Generate appropriate data visualizations for your key variables identified in the previous section\n",
    "* You should have at least three visualizations (and at least two different visualization types)\n",
    "* For each visualization provide an explanation regarding the variables involved and an interpretation of the graph.\n",
    "* If you are using Plotly, insert your visualizations as images as well (upload the graph images to an online source, e.g. github, and link those in Jupyter Notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Testing and Construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### 3.3. Model Construction\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import joblib\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/Users/zumaad/Laser_Translation/final/train.csv\")\n",
    "\n",
    "features  = data.drop(\"label\", axis = 1)\n",
    "target = data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=3000)\n",
    "\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "\n",
    "# model = DecisionTreeClassifier(max_depth = 10)\n",
    "\n",
    "# gnb = GaussianNB()\n",
    "\n",
    "# print(\"made models\")\n",
    "# print(\"\")\n",
    "\n",
    "# #k1.fit(X=X_train, y=y_train)\n",
    "# filename = 'Downloads/knn_model.sav'\n",
    "# joblib.dump(knn, filename)\n",
    "\n",
    "# model.fit(X=X_train, y=y_train)\n",
    "# print(\"fit DecisionTreeClassifier\")\n",
    "# print(\"\")\n",
    "\n",
    "# gnb.fit(X=X_train, y=y_train)\n",
    "# print(\"fit GaussianNB\")\n",
    "# print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "# print(\"testing the model with singular data\")\n",
    "# print(\"\")\n",
    "loaded_model = joblib.load(\"/Users/zumaad/Laser_Translation/final/k1_model.sav\")\n",
    "#indexes start from 14322 and has 10,500 numbers for test data. \n",
    "#indexes for eg are: 14322, 22264, 32118 etc.\n",
    "# use same number for index in x_test  and y_test: y_test  are the  answers  for x_test.\n",
    "\n",
    "\n",
    "d = X_test.loc[[21284]]\n",
    "\n",
    "# print(y_test[21284])\n",
    "# print(\"\")\n",
    "\n",
    "#model expects a data frame, the syntax above (passing a list to loc), returns a DF instead of a series representing a row\n",
    "print(loaded_model.predict(X_test.loc[[21284]])[0])\n",
    "# print(\"\")\n",
    "\n",
    "# if(y_test[21284] == loaded_model.predict(X_test.loc[[21284]])[0]):\n",
    "#     print(\"test passed\")\n",
    "# else:\n",
    "#     print(\"test failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.4. Model Evaluation\n",
    "* Evaluate the performance of your algorithms on appropriate evaluation metrics, using your validation set\n",
    "* Interpret your results from multiple models (and hypothesis tests, if any)\n",
    "\n",
    "print(\"calculating accuracies:\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with DecisionTreeClassifier :\", format(model.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with DecisionTreeClassifier :\", format(model.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with GaussianNB :\", format(gnb.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with GaussianNB :\", format(gnb.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with k1 :\", format(k1.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with k1 :\", format(k1.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Model Optimization\n",
    "* Tune your models using appropriate hyperparameters\n",
    "* Explain why you are doing this (e.g., to avoid overfitting, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Model Testing\n",
    "* Test your tuned algorithms using your testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Runthrough of Code - Final Script\n",
    "\n",
    "*Remember, everything is path specific. \n",
    "\n",
    "**1. The loaded model can be downloaded from , and should be linked in joblib.load(...)  \n",
    "**2. The test videos are labeled (...) and can be downloaded at ___. Please input the path of the video into the Video To ImageConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model from the file with all the model properties\n",
    "loaded_model = joblib.load(\"/Users/zumaad/Laser_Translation/final/k1_model.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert video to frames to be analyzed to create the composite image\n",
    "vti_converter = VideoToImageConverter('vid.mp4')\n",
    "vti_converter.splice_video('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object which is used to stitch individual frames into the composite image\n",
    "image_stitcher = ImageStitcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_array = image_stitcher.draw_image()\n",
    "pixel_dataframe = pd.DataFrame([pixel_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model.predict(pixel_dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DISCUSSION\n",
    "* Provide a summary of the steps you took to analyze your data and test your predictive model\n",
    "* Intepret your findings from 3.4., 3.5, and 3.6\n",
    "    * Which algorithms did you compare?\n",
    "    * Which algorithm(s) revealed best performance?\n",
    "    * Which algorithm(s) should be used for your predictive model?\n",
    "* If you tested hypotheses, interpret the results. What does it mean to have significant/non-significant differences with regards to your data?\n",
    "\n",
    "\n",
    "* End this section with a conclusion paragraph containing some pointers for future work \n",
    "    *(e.g., get more data, perform another analysis, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTRIBUTIONS\n",
    "* Describe each team member's contributions to the report (who did what in each section)\n",
    "* Remember this is a team effort!\n",
    "* Each member of your team will provide peer evaluation of other team members. Your final grade on the project will be based on those peer evaluations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
