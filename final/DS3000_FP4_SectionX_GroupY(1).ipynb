{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> DS 3000 - Spring 2019</h2> </center>\n",
    "<center> <h3> DS Report </h3> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h3> Lazer Translation</h3> </center>\n",
    "<center><h4>Srinath, Zumaad, Tushar, Eric</h4></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executive Summary:\n",
    "\n",
    "Dropbox links for required assets:\n",
    "https://www.dropbox.com/sh/hoixtm3q2ckzt71/AAB4EwdA0wU-ZyWZcd9YUDt8a?dl=0\n",
    "\n",
    "Our project was concerned with converting a video of someone drawing on a surface with a laser into a digit. To do this we had to convert a video into a representation of a digit that our model could understand. This involved splitting the video into frames/pixel arrays and creating an algorithm to create a composite pixel array which represented all the points the laser was at during the video.\n",
    "\n",
    "This composite pixel array was then passed to the model for prediction to get a digit. Our model was trained using KNN on a training set with around 30k images of digits and performed with 96.51% accuracy on the testing set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. <a href='#1'>INTRODUCTION</a>\n",
    "2. <a href='#2'>METHOD</a>\n",
    "3. <a href='#3'>RESULTS</a>\n",
    "4. <a href='#4'>DISCUSSION</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a proof of concept project that would allow us to convert laser drawn writing into digit representations. Through the completion of this endeavour we would have gained an in-depth exposure to python ski-kit lean’s utilities, have a greater understanding of using data sets to train models to recognize patterns such as handwriting, and achieve a comprehensive understanding of other python libraries such as image processing and pixel analysis. \n",
    "\n",
    "### Significance of the Problem\n",
    "Why is it important to tackle this problem in your project?\n",
    "In what ways could the insights from this project be useful?\n",
    "Has there been previous work on your topic? Do some research into your topic. Cite your sources appropriately. You can use the numbered reference format or APA (if you are more comfortable with it).\n",
    "This project serves as a proof of concept regarding the futuristic ability of medium translation from lasers to understandable text. Having a constant stream of data being read from a laser pointer and translated onto a screen not only greatly studies data science, machine learning, and image processing libraries, but will create an incredibly interesting project that can be constantly tweaked and improved. For example, one can extend the laser translation functionality to incorporate deciphering more than just digits - including symbols, shapes, and letters - and potentially bring light to a unique form of discrete communication. \n",
    "There has been no recorded research on our particular laser to text proposal. However, digit recognition is an incredibly widely researched subject, and one that many continue to discover new applications for [1]. We believe that our project signifies the creation of just one more of these useful applications.\n",
    "1. Team, D. F. (2020, January 7). Deep Learning Project - Handwritten Digit Recognition using\n",
    "Python. Retrieved from \n",
    "https://data-flair.training/blogs/python-deep-learning-project-handwritten-digit-recognition\n",
    "\n",
    "### Questions/Hypothesis\n",
    "\n",
    "\n",
    "Given the aforementioned problem and its importance, we set out to tackle the following questions:  \n",
    "\n",
    "Q1: Is the accuracy of our tests dependent on the robustness of our image stitching code or rather the model?  \n",
    "\n",
    "Q2: Will the speed of our laser drawn digit result in greater inaccuracy of digit prediction? (Requirement 2: variable question)  \n",
    "\n",
    "Q3: Due to the incredible size of the data set, what is the optimal algorithm and what are the most effective parameters for this algorithm, e.g., choosing the number of iterations, error tolerance, etc.?  \n",
    "\n",
    "\n",
    "We believe that the accuracy will be dependent on the color of laser used and the color of the background wall as well - making these decisions integral in guaranteeing success. In order to prove this, we will run our program through different wall backgrounds and analyze the implications of each.  \n",
    "\n",
    "Our hypothesis is that the accuracy of our tests will be fully determined by how robust our image stitching (i.e. the process that takes the parsed video images and re-constitutes a composite image by analyzing the points in each image with the greatest color contrast) code proves to be - as opposed to it being an issue on the model’s side. Our workflow will be to capture a video of someone outlining a digit with a laser pointer, splitting this video into various images each with their own laser location, stitching all of these images back together to form a singular number made up of various laser point locations, feeding this data into our digit-recognizer model, and finally printing this number.  Thus, our hypothesis is that the accuracy of this methodology will ultimately define the accuracy of our model and code as a whole.    \n",
    "\n",
    "Furthermore, we believe that the speed of the drawing will be a determining factor regarding the ultimate prediction accuracy of the drawn digit. Since we will be parsing the video drawing by frames, it would make sense that writer slower equates more accurate results.  \n",
    "\n",
    "Since numbers have a high variance we have a very large dataset, we hypothesis that the KNN algorithm will provide the most optimal results. Furthermore, the large number of features usually bogs down many accuracy algorithms and and makes training time unfeasibly long. This is the case with LinearSVC and not the case with KNN, so we have further reason to believe that KNN will be an optimal algorithm choice.\n",
    "\n",
    "\n",
    "\n",
    "…..\n",
    "\n",
    "*Side note regarding question requirements - Our feature variables are 784 pixels, and in of themselves, they don’t have any particular meaning. For example, in a heart disease prediction machine project - one could draw hypotheses based on a single variable (e.g. body-fat percentage) and what they believe its impact will be on the result. For our case, variable-based questions will have to reference outside factors such as speed of drawing, color of background, etc, since a single pixel density does not provide insight in of itself.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Acquisition\n",
    "\n",
    "Training data: kaggle.com/c/digit-recognizer/download/G4erCQmLsLsmveFfJfNg%2Fversions%2FFy8gTgxUjCWjkk6UfEsa%2Ffiles%2Ftrain.csv  \n",
    "\n",
    "Testing data: kaggle.com/c/digit-recognizer/download/G4erCQmLsLsmveFfJfNg%2Fversions%2FFy8gTgxUjCWjkk6UfEsa%2Ffiles%2Ftest.csv  \n",
    "\n",
    "The dataset contains 28,000 digit image representations ranging from 1 through 9 - each represented through a collection of 784 pixel values labeled pixel0, pixel1..pixel783. Each pixel variable provides a grayscale value (ranging from 0 to 255) that makes up the image. By taking one of these 784 long pixel series, reshaping it into a 28x28 image, and plotting the data through pyplot, the image representation of the digit is created. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Variables\n",
    "* For your hypotheses, what are your IVs and DVs?\n",
    "* For your predictive models, what are your features and target variables?\n",
    "\n",
    "**Independant variables**: Speed of the drawing, color of the laser, and the color of the wall.  \n",
    "**Dependant variables**: Accuracy of the prediction.\n",
    "\n",
    "**Features**: 784 pixels which have values ranging from 0-255 inclusive, where 255 is black and 0 is white.  \n",
    "**Target variables**: The digit (1-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Data Analysis\n",
    "Our target variable is the digit that the pixel array represents and it's value is based on the 784 feature variables.\n",
    "\n",
    "This is a supervised learning problem as our model was trained on labeled data (digits each corresponding to a 784 pixel values). The learning task was classification as it is predicting between a set number of target classes and not an infinite number as is done during regression tasks.\n",
    "\n",
    "We are going to use KNN, as its a high variance classifer and works best with larger datasets andafter testing the accuracies KNN had the highest accuracy. Furthermore, since we have a lot of features, other classification algorithms like linear SVC take too long to train relative to KNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Wrangling\n",
    "When training the model, we already had well formed data in a csv format that we could just read into a dataframe and train and test the model on. However, using the model with the rest of our workflow, we had to ultimately convert a video to a dataframe with 784 columns with each number in the column ranging from 0-255 where 255 is darkest and 0 is lightest, as that was the type of input the model needed. Doing this required extensive data wrangling.  \n",
    "\n",
    "To start off, we have a video of us drawing with a laser. We want to convert this to what represents an image of a digit. First, we have to split the video into frames and convert all of them into one image. Each frame shows the laser’s dot on the wall at a given time. So, to get the image of the digit we have to essentially overlay the laser point in each frame onto the final composite frame. To do this, we resize each frame so thats its 28 * 28 pixels (this is 784 pixels in totals like how the model expects). \n",
    "Then we get the pixel array of the image (a list of 784 items that range from 0-1, where 1 is darkest and 0 is lightest). To figure out the location of the laser point in that frame, we have a threshold: A green laser typically has a brightness less than .15, so we gather the locations of the pixels that are below that threshold and return them. These locations will be marked on the final composite pixel array as 255 (darkest) to mark where the laser’s point was.  \n",
    "\n",
    "After this process is done on every frame, the final composite pixel array has values 255 for every location the laser was, and 0 for all the other values. This array represents the digit. Finally, we convert it into a dataframe and pass it the model for prediction.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pre-processing of Video to Composite Pixel Image Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class PixelExtractor:\n",
    "    def __init__(self, image_path, color = 'g'):\n",
    "        \"\"\"Image represents file path to target file \"\"\"\n",
    "        self.image_path = image_path\n",
    "        self.color = color\n",
    "        \n",
    "        \n",
    "    def imageprepare(self, argv):\n",
    "        \"\"\"\n",
    "        This function returns the pixel values as one array with 784 pixel values normalized\n",
    "        so that 255 is 1 and 0 is 0.\n",
    "        \"\"\"\n",
    "         \n",
    "        im = Image.open(argv).convert('L')\n",
    "        img = im.resize((28, 28), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        tv = list(img.getdata()) \n",
    "      \n",
    "        # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "        tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    "        return tva\n",
    "\n",
    "    def reshape_pixel_array(self, pixel_arr):\n",
    "        \"\"\" Takes flat array of 784 values and turns it into a 2d array with 28 rows of size 28 \"\"\"\n",
    "        reshaped_pixel_arr = []\n",
    "        n = 28\n",
    "        while n <= len(pixel_arr):\n",
    "            reshaped_pixel_arr.append(pixel_arr[n-28:n])\n",
    "            n+=28\n",
    "\n",
    "        return reshaped_pixel_arr\n",
    "    \n",
    "    def extract_target_pixel_location(self):\n",
    "        \"\"\" Returns list of target pixel locations \"\"\"\n",
    "        #Respective Image location\n",
    "        pixel_array = self.imageprepare(self.image_path)\n",
    "\n",
    "        #Select less_than_target color point --> must be calibrated\n",
    "        #?? Should we use an abstract class here instead of an if statment ??\n",
    "        if self.color == \"g\":\n",
    "            less_than_target = .15\n",
    "        else:\n",
    "            raise ValueError(\"Unknown color value\")\n",
    "\n",
    "        #Chooses target pixels as well as it's location\n",
    "        target_pixels = []\n",
    "        for pixel in enumerate(pixel_array):\n",
    "            if pixel[1] < less_than_target:\n",
    "                target_pixels.append(pixel[0])\n",
    "\n",
    "        return target_pixels\n",
    "    \n",
    "    def draw_image(self):\n",
    "        \"\"\" Draws the image representation of the rgb pixel valued image \"\"\"\n",
    "      \n",
    "        pixel_array = self.imageprepare(self.image_path)\n",
    "        newArr = self.reshape_pixel_array(pixel_array)\n",
    "        plt.imshow(newArr, interpolation='nearest')\n",
    "        plt.savefig('MNIST_IMAGE.png')#save MNIST image\n",
    "        plt.show()#Show / plot that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class ImageStitcher:\n",
    "    def __init__(self, directory_path = 'data'):\n",
    "        \"\"\" Directory Path represents directory containing spliced images \"\"\"\n",
    "\n",
    "        self.directory_path = directory_path\n",
    "        \n",
    "    def create_composite_image_list (self):\n",
    "        \"\"\" \n",
    "        Takes a directory and overlays its content images together based on \n",
    "        PixelExtractor's conditions\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        directory = os.fsencode(self.directory_path)\n",
    "        #255 represents a black cell\n",
    "        #0 represents a white cell\n",
    "        composite_image_list = [0 for i in range(784)]\n",
    "\n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "\n",
    "            #creates pixel_extractor instance\n",
    "            pixel_extractor = PixelExtractor(self.directory_path + '/' + filename, 'g')\n",
    "            target_pixel_locations = pixel_extractor.extract_target_pixel_location()\n",
    "            for loc in target_pixel_locations:\n",
    "                composite_image_list[loc] = 255\n",
    "\n",
    "        return composite_image_list\n",
    "    \n",
    "    def draw_image(self):\n",
    "        \"\"\"Draws the image representation of the composite image\"\"\"\n",
    "\n",
    "        composite_image_list = self.create_composite_image_list()\n",
    "\n",
    "        reshaped_composite_image = self.reshape_pixel_array(composite_image_list)\n",
    "\n",
    "        plt.imshow(reshaped_composite_image, cmap='Greys',  interpolation='nearest')\n",
    "        # plt.savefig('MNIST_IMAGE.png')#save MNIST image\n",
    "        plt.show()#Show / plot that image\n",
    "        return composite_image_list\n",
    "    \n",
    "    def reshape_pixel_array(self, composite_image_list):\n",
    "        \"\"\" Takes flat array of 784 values and turns it into a 2d array with 28 rows of size 28 \"\"\"\n",
    "        reshaped_composite_image = []\n",
    "        n = 28\n",
    "        while n <= len(composite_image_list):\n",
    "            reshaped_composite_image.append(composite_image_list[n-28:n])\n",
    "            n+=28\n",
    "\n",
    "        return reshaped_composite_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "class VideoToImageConverter:\n",
    "    \"\"\"\n",
    "    This class is responsible for converting a video to frames and saving it\n",
    "    to a specified directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_path):\n",
    "        self.video_path = video_path\n",
    "\n",
    "    def splice_video(self, destination_path = 'data'):\n",
    "        \"\"\" \n",
    "        Splices given video to individual images and \n",
    "        writes them to a folder specified by destination_path\n",
    "\n",
    "        *Don't name your destination path a folder that is important,\n",
    "        since this folder will be deleted and populated with images\n",
    "        \"\"\"\n",
    "\n",
    "        # Playing video from file:\n",
    "        vidcap = cv2.VideoCapture(self.video_path)\n",
    "\n",
    "        #Saves to respective folder\n",
    "        try:\n",
    "            if os.path.exists(destination_path):\n",
    "                #Deletes any folder currently named ./data if it exists\n",
    "                shutil.rmtree(destination_path, ignore_errors=True)\n",
    "                print(\"Deleting current '\" + destination_path + \"' folder\" )\n",
    "\n",
    "\n",
    "            print(\"Creating new '\" + destination_path + \"' folder\" )\n",
    "            os.makedirs(destination_path)\n",
    "\n",
    "        except OSError:\n",
    "            print ('Error: Cannot create directory')\n",
    "\n",
    "        frame_count = 0\n",
    "        print(\"processing frames, this takes around 10 seconds\")\n",
    "        while(True):\n",
    "            # Capture frame-by-frame\n",
    "\n",
    "            hasFrames,image = vidcap.read()\n",
    "\n",
    "            if hasFrames:\n",
    "\n",
    "                # Saves image of the current frame in jpg file\n",
    "                name = './'+ destination_path +'/frame' + str(frame_count) + '.jpg'\n",
    "                cv2.imwrite(name, image)\n",
    "                frame_count += 1\n",
    "\n",
    "            else:\n",
    "                print(\"done processing frames\")\n",
    "                break\n",
    "\n",
    "        # When everything done, release the capture\n",
    "        vidcap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data Exploration\n",
    "* Generate appropriate data visualizations for your key variables identified in the previous section\n",
    "* You should have at least three visualizations (and at least two different visualization types)\n",
    "* For each visualization provide an explanation regarding the variables involved and an interpretation of the graph.\n",
    "* If you are using Plotly, insert your visualizations as images as well (upload the graph images to an online source, e.g. github, and link those in Jupyter Notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3. Model Construction and training\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/tdesai2017/Laser_Translation/master/final/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating models\n",
      "training all the models\n",
      "fitting k1\n",
      "successfully fit k1\n",
      "\n",
      "fitting k2\n",
      "successfully fit k2\n",
      "\n",
      "fitting KNeighborsClassifier\n",
      "Successfully fit KNeighborsClassifier\n",
      "\n",
      "fitting DecisionTreeClassifier\n",
      "succesfully fit DecisionTreeClassifier\n",
      "\n",
      "fitting GuassianNB\n",
      "Successfully fit GaussianNB\n",
      "\n",
      "fitting SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fit SGD classifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features  = data.drop(\"label\", axis = 1)\n",
    "target = data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=3000)\n",
    "\n",
    "print(\"creating models\")\n",
    "\n",
    "k1 = KNeighborsClassifier(n_neighbors = 3, metric='euclidean')\n",
    "\n",
    "k2 = KNeighborsClassifier(n_neighbors = 4)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(max_depth = 10)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "SGDModel = SGDClassifier()\n",
    "\n",
    "\n",
    "print(\"training all the models\")\n",
    "\n",
    "print(\"fitting k1\")\n",
    "k1.fit(X=X_train, y=y_train)\n",
    "print(\"successfully fit k1\\n\")\n",
    "\n",
    "print(\"fitting k2\")\n",
    "k2.fit(X=X_train, y=y_train)\n",
    "print(\"successfully fit k2\\n\")\n",
    "\n",
    "print(\"fitting KNeighborsClassifier\")\n",
    "knn.fit(X=X_train, y=y_train)\n",
    "print(\"Successfully fit KNeighborsClassifier\\n\")\n",
    "\n",
    "print(\"fitting DecisionTreeClassifier\") \n",
    "decision_tree.fit(X=X_train, y=y_train)\n",
    "print(\"succesfully fit DecisionTreeClassifier\\n\")\n",
    "\n",
    "print(\"fitting GuassianNB\")\n",
    "gnb.fit(X=X_train, y=y_train)\n",
    "print(\"Successfully fit GaussianNB\\n\")\n",
    "\n",
    "print(\"fitting SGD\")\n",
    "SGDModel.fit(X=X_train, y=y_train)\n",
    "print(\"Successfully fit SGD classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating accuracies:\n",
      "\n",
      "Prediction accuracy on the training data with DecisionTreeClassifier : 90.80\n",
      "Prediction accuracy on the test data with DecisionTreeClassifier : 84.12\n",
      "\n",
      "Prediction accuracy on the training data with GaussianNB : 56.23\n",
      "Prediction accuracy on the test data with GaussianNB : 55.26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 3.4. Model Evaluation\n",
    "print(\"calculating accuracies:\\n\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with DecisionTreeClassifier :\", format(decision_tree.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with DecisionTreeClassifier :\", format(decision_tree.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with GaussianNB :\", format(gnb.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with GaussianNB :\", format(gnb.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with k1 :\", format(knn.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with k1 :\", format(knn.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with SGDClassifier :\", format(SGDModel.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with SGDClassifier :\", format(SGDModel.score(X_test, y_test)*100, \".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.5. Model Optimization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X=X_train, y=y_train)\n",
    "\n",
    "param_grid = {\"n_neighbors\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X=X_train, y=y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Training set score with best parameters: \", grid_search.score(X_train, y_train))\n",
    "print(\"Test set score with best parameters: \", grid_search.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.6. Model Testing\n",
    "\n",
    "#testing the knn model that we optimized (k1) against the ones we didn't (k2 and knn)\n",
    "\n",
    "print(\"Prediction accuracy on the training data with k1 :\", format(k1.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with k1 :\", format(k1.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with k2 :\", format(k2.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with k2 :\", format(k2.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Prediction accuracy on the training data with KNeighborsClassifier :\", format(knn.score(X_train, y_train)*100, \".2f\"))\n",
    "print(\"Prediction accuracy on the test data with KNeighborsClassifier :\", format(knn.score(X_test, y_test)*100, \".2f\"))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Runthrough of Code - Final Script\n",
    "\n",
    "*Remember, everything is path specific. \n",
    "\n",
    "**1. The loaded model can be downloaded from , and should be linked in joblib.load(...)  \n",
    "**2. The test videos are labeled (...) and can be downloaded at ___. Please input the path of the video into the Video To ImageConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.21.3 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#load the model from the file with all the model properties\n",
    "\n",
    "#you can download this from the dropbox link we provided, or you can train the models above and use them.\n",
    "loaded_model = joblib.load(\"k1_model.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current 'data' folder\n",
      "Creating new 'data' folder\n",
      "processing frames, this takes around 10 seconds\n",
      "done processing frames\n"
     ]
    }
   ],
   "source": [
    "#convert video to frames to be analyzed to create the composite image\n",
    "vti_converter = VideoToImageConverter('vid.mp4')\n",
    "vti_converter.splice_video('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object which is used to stitch individual frames into the composite image\n",
    "image_stitcher = ImageStitcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#drawing the image takes around 5 seconds\n",
    "pixel_array = image_stitcher.draw_image()\n",
    "pixel_dataframe = pd.DataFrame([pixel_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.predict(pixel_dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DISCUSSION\n",
    "* Provide a summary of the steps you took to analyze your data and test your predictive model\n",
    "\n",
    "We read out data from a csv and split it into training and testing data. We constructed four different classification models (SGD, Decision tree, GNB, and KNN) and evaluated their performance. KNN performed the best so we decided to optimize it further. \n",
    "\n",
    "To optimize KNN we ran GridSearch on it and figured out the best parameters where a neighbor count of 3 and a metric of euclidean. The GridSearch optimized KNN had 96.51% accuracy on test data and 98.28% accuracy on training data. Prior to GridSearch, it had 97.41% accuracy on training data, and 96.51% accuracy on test data.\n",
    "\n",
    "\n",
    "* Intepret your findings from 3.4., 3.5, and 3.6\n",
    "\n",
    "We compared KNN, GNB, Decision tree, and SGD. KNN had the best performance and improved slightly after applying grid search to it. We should use KNN as it has the best accuracy and that is what we decided to go with.\n",
    "    \n",
    "    \n",
    "* If you tested hypotheses, interpret the results. What does it mean to have significant/non-significant differences with regards to your data?\n",
    "\n",
    "We had several hypothesis.\n",
    "\n",
    "We thought that the speed of the laser drawing was a crucial factor. This was true as shown by the lack luster interpretation of a hastily drawn number:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current 'data' folder\n",
      "Creating new 'data' folder\n",
      "processing frames, this takes around 10 seconds\n",
      "done processing frames\n"
     ]
    }
   ],
   "source": [
    "vti_converter = VideoToImageConverter('fast3.mp4')\n",
    "vti_converter.splice_video('data')\n",
    "image_stitcher = ImageStitcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pixel_array = image_stitcher.draw_image()\n",
    "pixel_dataframe = pd.DataFrame([pixel_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.predict(pixel_dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see drawing fast causes really poor quality images. Here is another test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current 'data' folder\n",
      "Creating new 'data' folder\n",
      "processing frames, this takes around 10 seconds\n",
      "done processing frames\n"
     ]
    }
   ],
   "source": [
    "vti_converter = VideoToImageConverter('anotherfast3.mp4')\n",
    "vti_converter.splice_video('data')\n",
    "image_stitcher = ImageStitcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pixel_array = image_stitcher.draw_image()\n",
    "pixel_dataframe = pd.DataFrame([pixel_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In second test, yet another poor quality image was produced. Here is the model's prediction based on the iamge above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.predict(pixel_dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second hypothesis:\n",
    "We thought the color of the wall and the laser would impact the accuracy. This was correct, but it is nuanced in that darker backgrounds work better due to our algorithm looking for pixels with a darkness lower than a certain treshold. Hence, with darker backgrounds there are less false positives as the only pixels with a higher brightness/lower darkness are the ones with the laser dot on them. This is demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current 'data' folder\n",
      "Creating new 'data' folder\n",
      "processing frames, this takes around 10 seconds\n",
      "done processing frames\n"
     ]
    }
   ],
   "source": [
    "#on a red (darker) background\n",
    "vti_converter = VideoToImageConverter('redbackground3.mp4')\n",
    "vti_converter.splice_video('data')\n",
    "image_stitcher = ImageStitcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pixel_array = image_stitcher.draw_image()\n",
    "pixel_dataframe = pd.DataFrame([pixel_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still works correctly as the background is darker than the laser and only the laser's locations\n",
    "are bright enough to make the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current 'data' folder\n",
      "Creating new 'data' folder\n",
      "processing frames, this takes around 10 seconds\n",
      "done processing frames\n"
     ]
    }
   ],
   "source": [
    "#on a brighter background\n",
    "vti_converter = VideoToImageConverter('lightbackground3.mp4')\n",
    "vti_converter.splice_video('data')\n",
    "image_stitcher = ImageStitcher()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pixel_array = image_stitcher.draw_image()\n",
    "pixel_dataframe = pd.DataFrame([pixel_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This messes up as the background is lighter than the laser in certain spots (the middle) and as such the algorithm sees that those pixels are bright enough to be considered the laser point and marks them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current 'data' folder\n",
      "Creating new 'data' folder\n",
      "processing frames, this takes around 10 seconds\n",
      "done processing frames\n"
     ]
    }
   ],
   "source": [
    "#this is a test on a non uniform background\n",
    "vti_converter = VideoToImageConverter('weirdbackground3.mp4')\n",
    "vti_converter.splice_video('data')\n",
    "image_stitcher = ImageStitcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_array = image_stitcher.draw_image()\n",
    "pixel_dataframe = pd.DataFrame([pixel_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other Hypothesis was that KNN will be the best classification algorithm to use and it was as demonstrated in the model evaluation and model testing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* End this section with a conclusion paragraph containing some pointers for future work \n",
    "    *(e.g., get more data, perform another analysis, etc.)\n",
    "    \n",
    "There are several ways we can take this project and improve on it in the future. Though the model's accuracy is really good on well formed digits, it doesn't perform nearly as well on the ragged digits that are produced through lazer drawing. Perhaps training the model on less well formed labelled digits will prove better for accuracy on the types of input we will realistically be giving it.\n",
    "\n",
    "Another idea is feeding the model data in realtime as the frames come in, instead of needing a saved video first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTRIBUTIONS\n",
    "**Tushar**: Worked on the image processing pipeline/orchestration and image stitching.  \n",
    "**Zumaad**: Worked on the image processing pipeline/orchestration and image stitching.  \n",
    "**Srinath**: worked on model construction, evaluation and visualization.  \n",
    "**Eric**: worked on the conversion from video into frames with variable frame speeds.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
